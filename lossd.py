import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib
import os
matplotlib.use('Agg')  # ä½¿ç”¨éå›¾å½¢ç•Œé¢åç«¯ï¼ˆä¸å¼¹çª—ï¼Œå¯ä¿å­˜ï¼‰
import matplotlib.pyplot as plt
import math

# äºŒå€¼åŒ–å›¾ç™¾åˆ†æ¯”
def print_mask_distribution(pred_bin_mask):


    print(f"ğŸ¯ pred_bin_mask ä¸­åƒç´ å€¼åˆ†å¸ƒï¼š")
    print(f"  1ï¼ˆå‰æ™¯ï¼‰: {num_ones} åƒç´ ï¼Œå æ¯” {pct_ones:.2f}%")
    print(f"  0ï¼ˆèƒŒæ™¯ï¼‰: {num_zeros} åƒç´ ï¼Œå æ¯” {pct_zeros:.2f}%")

# äºŒå€¼åŒ–å›¾å¯è§†åŒ–
def save_binary_mask(pred_bin_mask, save_path="pred_bin_mask.png"):

    print(f"âœ… å·²ä¿å­˜ä¸ºé»‘ç™½å›¾ï¼š{save_path}")

# äºŒå€¼åŒ–æ£€æŸ¥
def assert_binary_mask(mask: torch.Tensor, label=None):
    """
    æ£€æŸ¥ mask æ˜¯å¦ä¸¥æ ¼ä¸ºäºŒå€¼ï¼ˆ0 æˆ– 1ï¼‰ï¼Œå¦åˆ™æŠ›å‡ºå¼‚å¸¸ã€‚
    Args:
        mask: Tensorï¼Œå½¢çŠ¶ä¸º [B, 1, H, W]
        label: å¯é€‰ï¼ŒæŠ¥é”™æ—¶è¾“å‡ºæ˜¯å“ªä¸€ç±»
    """

    if not torch.all((unique_vals == 0) | (unique_vals == 1)):
        raise ValueError(f"âŒ pred_bin_mask ä¸­å­˜åœ¨é 0/1 å€¼: {unique_vals.tolist()} (label={label})")


def manhattan_distance_soft_limited(mask: torch.Tensor, max_iter: int = 50) -> torch.Tensor:
    """
    å¯¹ binary mask ä¼ æ’­æ¯ç‚¹åˆ°æœ€è¿‘èƒŒæ™¯ç‚¹ï¼ˆå€¼ä¸º0ï¼‰çš„æ›¼å“ˆé¡¿è·ç¦»ï¼Œé™åˆ¶ä¼ æ’­æ­¥æ•°ã€‚
    å¦‚æœä¼ æ’­åˆ° max_iter æ­¥è¿˜æ²¡é‡åˆ°èƒŒæ™¯ï¼Œè·ç¦»å°±ä¿æŒä¸º max_iterã€‚

    Args:
        mask: [B, 1, H, W]ï¼Œbinary maskï¼Œå‰æ™¯=1ï¼ŒèƒŒæ™¯=0
        max_iter: æœ€å¤§ä¼ æ’­æ­¥æ•°

    Returns:
        dist: [B, 1, H, W]ï¼Œæ¯ä¸ªå‰æ™¯åƒç´ ç‚¹åˆ°æœ€è¿‘èƒŒæ™¯çš„æ›¼å“ˆé¡¿è·ç¦»
    """


    # Step 2: æ›¼å“ˆé¡¿ä¼ æ’­


    return dist

# ç”Ÿæˆæ­£è´Ÿæ–¹å‘æ€§è·ç¦»å›¾
def directional_distance_maps(gt_bin_mask: torch.Tensor, max_iter: int = 50) -> torch.Tensor:
    """
    æ„é€ æ–¹å‘æ€§è·ç¦»å›¾ï¼ˆæ­£å€¼ï¼šæ©ç å†…éƒ¨è·ç¦»ï¼Œè´Ÿå€¼ï¼šæ©ç å¤–éƒ¨è¶Šç•Œè·ç¦»ï¼‰
    Args:
        gt_bin_mask: [B, 1, H, W]ï¼ŒçœŸå®æ ‡ç­¾äºŒå€¼æ©ç     1 1 1 1 0 0 0 0
        max_iter: æœ€å¤§ä¼ æ’­æ­¥æ•°
    Returns:
        full_dist: [B, 1, H, W]ï¼Œå¸¦ç¬¦å·è·ç¦»å›¾
    """

    return gt_dist_pos, full_dist

# ç”Ÿæˆä¼ªone - hot ç¼–ç 
def ste_one_hot_from_logits(logits: torch.Tensor):
    """
    ä½¿ç”¨ Straight-Through Estimator å®ç°çš„å¯å¯¼ one-hot é¢„æµ‹ã€‚

    Args:
        logits: [B, C, H, W] åŸå§‹ç½‘ç»œè¾“å‡º
    Returns:
        pred_class: [B, H, W] éå¯å¯¼é¢„æµ‹ç±»åˆ«ç´¢å¼•ï¼ˆfor visualizationï¼‰
        ste_mask:   [B, C, H, W] å¯å¯¼çš„ one-hot è¿‘ä¼¼ï¼ˆfor trainingï¼‰
    """
    # Step 1: softmax å¾—åˆ°æ¦‚ç‡

    return pred_class, ste_mask

# å½’ä¸€åŒ–ï¼ˆä¸¤ä¸ªå‚æ•°æ§åˆ¶ï¼‰
def normalize_log_triangular_lossvorg(diff: torch.Tensor, max_n: int = 10, clip_n: int = 20, eps: float = 1e-6):
    """
    ä½¿ç”¨ log(1 + loss) å¯¹ä¸‰è§’å½¢+çº¿æ€§å¢é•¿æŸå¤±è¿›è¡Œå½’ä¸€åŒ–å‹ç¼©ï¼Œè¿”å›èŒƒå›´åœ¨ [0, 1]
    """


    return norm_loss

# å½’ä¸€åŒ–ï¼Œè‡ªåŠ¨è®¾ç½®æˆªå–ä¸º2å€æœ€å¤§å€¼ é’ˆå¯¹L1
def normalize_log_triangular_loss_smart(diff: torch.Tensor, max_n: int = 20, eps: float = 1e-6):
    """
    ä¸‰è§’+çº¿æ€§å¢é•¿å½¢å¼çš„æŸå¤±å‡½æ•°çš„ log å½’ä¸€åŒ–ç‰ˆæœ¬
    Args:
        diff: é¢„æµ‹ä¸ç›®æ ‡å·®å€¼ |pred - target|ï¼ŒTensor
        max_n: æ‹ç‚¹ï¼Œä¸‰è§’å¢é•¿ â†’ çº¿æ€§å¢é•¿çš„ä¸´ç•Œç‚¹
        eps: é¿å…é™¤ä»¥ 0 çš„å¾®å°æ•°

    Returns:
        norm_loss: [B, 1, H, W]ï¼Œå½’ä¸€åŒ–åçš„ loss å€¼ï¼ŒèŒƒå›´ [0, 1]
    """


    return norm_loss


def normalize_log(diff, max_n=10, eps=1e-6):
    """
    norm(d) = log(1 + |d|) / log(1 + max_n)
    """
    safe_diff = diff.abs() + eps
    return torch.log1p(safe_diff) / math.log1p(max_n)  # è¾“å‡ºå½¢çŠ¶ ä»æ˜¯ [B, 1, H, W]


class DistanceLoss(nn.Module):
    def __init__(self, label_list, label_weight=None, max_iter=50, p=1, reduction='mean'):
        """
        Args:
            label_list: å‚ä¸è®¡ç®—çš„ç±»åˆ«åˆ—è¡¨ï¼Œå¦‚ [1, 2, 3, 4]
            label_weight: æ¯ä¸ªç±»åˆ«çš„æŸå¤±æƒé‡ï¼Œlist ç±»å‹ï¼Œä¸ label_list å¯¹åº”
            max_iter: æœ€å¤§ä¼ æ’­æ­¥æ•°
            p: L1ï¼ˆ1ï¼‰æˆ– L2ï¼ˆ2ï¼‰è·ç¦»
            reduction: 'mean' or 'sum'
        """
        super().__init__()
        self.label_list = label_list
        self.label_weight = label_weight if label_weight is not None else [1.0] * len(label_list)
        assert len(self.label_list) == len(self.label_weight), "label_list å’Œ label_weight é•¿åº¦ä¸ä¸€è‡´ï¼"

        self.max_iter = max_iter
        self.p = p
        self.reduction = reduction
        self.loss_fn = nn.L1Loss() if p == 1 else nn.MSELoss()

    def forward(self, logits: torch.Tensor, target: torch.Tensor) -> torch.Tensor:
        pred_class, ste_mask = ste_one_hot_from_logits(logits)

        total_loss = 0.0
        valid_class_count = 0

        # ç­›é€‰å‡ºç°çš„ç±»åˆ« & å±äº label_list çš„
        pred_labels = pred_class.unique()  # é¢„æµ‹ä¸­å‡ºç°çš„ç±»åˆ«
        true_labels = target.unique()      # æ ‡ç­¾ä¸­å‡ºç°çš„ç±»åˆ«
        all_labels = torch.unique(torch.cat([pred_labels, true_labels], dim=0))  # åˆå¹¶åå»é‡å¤

        allowed_labels = torch.tensor(self.label_list, device=logits.device)
        all_labels = all_labels[torch.isin(all_labels, allowed_labels)]

        for label in all_labels:
            label = int(label.item())
            label_idx = self.label_list.index(label)
            weight = self.label_weight[label_idx]

            pred_bin_mask = ste_mask[:, label:label+1, :, :]   #  ä» ste_mask ä¸­æå–æŒ‡å®š label çš„é€šé“ï¼Œä½œä¸ºè¯¥ç±»çš„é¢„æµ‹äºŒå€¼å›¾
            assert_binary_mask(pred_bin_mask, label)
            # print("pred_bin_mask unique values:", torch.unique(pred_bin_mask))  # æ‰“å° çœ‹é‡Œé¢å‡ºç°çš„æ˜¯ä¸æ˜¯1/0
            # print_mask_distribution(pred_bin_mask)  # è®¡ç®—1/0ç™¾åˆ†æ¯”
            # save_binary_mask(pred_bin_mask, save_path="pred_bin_mask.png")  # å¯è§†åŒ–

            target_bin_mask = (target == label).float().unsqueeze(1)

            if pred_bin_mask.sum() < 1e-6 and target_bin_mask.sum() < 1e-6:
                continue

            gt_dist_pos, full_dist = directional_distance_maps(target_bin_mask, self.max_iter)
            pred_dist = pred_bin_mask * full_dist  # ä»…æå–é¢„æµ‹åŒºåŸŸçš„è·ç¦»å€¼ï¼ˆä¸ºæ­£æˆ–è´Ÿï¼‰ # 4 3 2 1 -1 -2 0 0

            # loss = self.loss_fn(pred_dist, gt_dist_pos)
            diff = torch.abs(pred_dist - gt_dist_pos)  # 0 0 0 0 -1 -2

            # å¯¹è¿™ä¸ªå·®å€¼å›¾åšå½’ä¸€åŒ–
            norm_diff = normalize_log(diff, max_n=self.max_iter)  # è¾“å‡ºå½¢çŠ¶ ä»æ˜¯ [B, 1, H, W]

            # ç„¶åå†èšåˆï¼ˆæ•´å¼ å›¾ä¸Šçš„åƒç´ æ±‚å¹³å‡ï¼‰
            loss = norm_diff.mean()

            # # é¢„æµ‹å‰æ™¯æ±‚å¹³å‡ï¼Œæœ‰é—®é¢˜ï¼Œå¯èƒ½é¢„æµ‹å‰æ™¯å…¨ä¸º0
            # nonzero_mask = (pred_bin_mask == 1).float()
            # loss = (norm_diff * nonzero_mask).sum() / (nonzero_mask.sum() + 1e-6)

            # çœŸå®å‰æ™¯æ±‚å¹³å‡  æœ‰é—®é¢˜ï¼Œå¯èƒ½çœŸå®å‰æ™¯ä¸ºå…¨0
            # target_mask = (target_bin_mask == 1).float()
            # loss = (norm_diff * pred_bin_mask).sum() / (target_mask.sum() + 1e-6)

            total_loss += weight * loss
            valid_class_count += 1

        if valid_class_count == 0:
            return logits.sum() * 0  # ä¿è¯æ¢¯åº¦å­˜åœ¨

        return total_loss / valid_class_count if self.reduction == 'mean' else total_loss